{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using config file conf_1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from joblib import Parallel, delayed\n",
    "# import seaborn as sns\n",
    "import scipy as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "sys.argv = ['--config', 'conf_1']\n",
    "# sys.argv = ['--config', 'config1']\n",
    "\n",
    "from models import *\n",
    "from loss import *\n",
    "from run import *\n",
    "import random\n",
    "# from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config0:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='tf_efficientnet_b1_ns'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/tf_efficientnet_b1_ns_best_fold_0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config1:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='tf_efficientnet_b1_ns'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/tf_efficientnet_b1_ns_best_fold_1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config2:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='tf_efficientnet_b1_ns'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/tf_efficientnet_b1_ns_best_fold_2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config3:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='tf_efficientnet_b1_ns'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/tf_efficientnet_b1_ns_best_fold_3.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config4:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='tf_efficientnet_b1_ns'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/tf_efficientnet_b1_ns_best_fold_4.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config5:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='dm_nfnet_f0'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/dm_nfnet_f0_best_fold_0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config6:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='dm_nfnet_f0'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/dm_nfnet_f0_best_fold_1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config7:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='dm_nfnet_f0'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/dm_nfnet_f0_best_fold_2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config8:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='dm_nfnet_f0'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/dm_nfnet_f0_best_fold_3.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config9:\n",
    "    DEBUG=False\n",
    "    num_workers=8\n",
    "    gpus='0'\n",
    "    distributed_backend=None\n",
    "    sync_batchnorm=True\n",
    "    channels_last=False\n",
    "    gradient_accumulation_steps=4\n",
    "    precision=16\n",
    "    warmup_epo=1\n",
    "    cosine_epo=19\n",
    "    lr=0.002\n",
    "    weight_decay=0.0001\n",
    "    p_trainable=True\n",
    "    crit='bce'\n",
    "    backbone='dm_nfnet_f0'\n",
    "    embedding_size=512\n",
    "    pool='gem'\n",
    "    arcface_s=45.0\n",
    "    arcface_m=0.4\n",
    "    neck='option-D'\n",
    "    head='arc_margin'\n",
    "    pretrained_weights=None\n",
    "    optim='sgd'\n",
    "    batch_size=12\n",
    "    n_splits=5\n",
    "    fold=0\n",
    "    seed=1028\n",
    "    device='cuda:0'\n",
    "    out_dim=1049\n",
    "    n_classes=1000\n",
    "    class_weights='log'\n",
    "    class_weights_norm='batch'\n",
    "    normalization='imagenet'\n",
    "    crop_size=448\n",
    "    model='../model/dm_nfnet_f0_best_fold_4.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LMDataset(Dataset): \n",
    "    def __init__(self, csv): \n",
    "        self.csv = csv.filepath.values\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.csv[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img1 = val_aug1(image=img)['image'].astype(np.float32)\n",
    "        img1 = self.normalize_img(img1)\n",
    "        tensor1 = self.to_torch_tensor(img1)\n",
    "        feature_dict1 = {'idx':torch.tensor(index).long(), 'input':tensor1}\n",
    "\n",
    "        img2 = val_aug2(image=img)['image'].astype(np.float32)\n",
    "        img2 = self.normalize_img(img2)\n",
    "        tensor2 = self.to_torch_tensor(img2)\n",
    "        feature_dict2 = {'idx':torch.tensor(index).long(), 'input':tensor2}\n",
    "\n",
    "        img3 = val_aug3(image=img)['image'].astype(np.float32)\n",
    "        img3 = self.normalize_img(img3)\n",
    "        tensor3 = self.to_torch_tensor(img3)\n",
    "        feature_dict3 = {'idx':torch.tensor(index).long(), 'input':tensor3}\n",
    "\n",
    "        img4 = val_aug4(image=img)['image'].astype(np.float32)\n",
    "        img4 = self.normalize_img(img4)\n",
    "        tensor4 = self.to_torch_tensor(img4)\n",
    "        feature_dict4 = {'idx':torch.tensor(index).long(), 'input':tensor4}\n",
    "\n",
    "        img5 = val_aug5(image=img)['image'].astype(np.float32)\n",
    "        img5 = self.normalize_img(img5)\n",
    "        tensor5 = self.to_torch_tensor(img5)\n",
    "        feature_dict5 = {'idx':torch.tensor(index).long(), 'input':tensor5}\n",
    "\n",
    "        img6 = val_aug6(image=img)['image'].astype(np.float32)\n",
    "        img6 = self.normalize_img(img6)\n",
    "        tensor6 = self.to_torch_tensor(img6)\n",
    "        feature_dict6 = {'idx':torch.tensor(index).long(), 'input':tensor6}\n",
    "\n",
    "        img7 = val_aug7(image=img)['image'].astype(np.float32)\n",
    "        img7 = self.normalize_img(img7)\n",
    "        tensor7 = self.to_torch_tensor(img7)\n",
    "        feature_dict7 = {'idx':torch.tensor(index).long(), 'input':tensor7}\n",
    "\n",
    "        img8 = val_aug8(image=img)['image'].astype(np.float32)\n",
    "        img8 = self.normalize_img(img8)\n",
    "        tensor8 = self.to_torch_tensor(img8)\n",
    "        feature_dict8 = {'idx':torch.tensor(index).long(), 'input':tensor8}\n",
    "        \n",
    "        \n",
    "        img9 = val_aug9(image=img)['image'].astype(np.float32)\n",
    "        img9 = self.normalize_img(img9)\n",
    "        tensor9 = self.to_torch_tensor(img9)\n",
    "        feature_dict9 = {'idx':torch.tensor(index).long(), 'input':tensor9}\n",
    "\n",
    "        img10 = val_aug10(image=img)['image'].astype(np.float32)\n",
    "        img10 = self.normalize_img(img10)\n",
    "        tensor10 = self.to_torch_tensor(img10)\n",
    "        feature_dict10 = {'idx':torch.tensor(index).long(), 'input':tensor10}\n",
    "\n",
    "        \n",
    "\n",
    "        return feature_dict1, feature_dict2, feature_dict3 , feature_dict4, feature_dict5, feature_dict6, feature_dict7, feature_dict8, feature_dict9, feature_dict10\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.csv)\n",
    "    def normalize_img(self,img):\n",
    "        mean = np.array([123.675, 116.28 , 103.53 ], dtype=np.float32)\n",
    "        std = np.array([58.395   , 57.120, 57.375 ], dtype=np.float32)\n",
    "        img = img.astype(np.float32)\n",
    "        img -= mean\n",
    "        img *= np.reciprocal(std, dtype=np.float32)\n",
    "        return img\n",
    "    def to_torch_tensor(self,img):\n",
    "        return torch.from_numpy(img.transpose((2, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aug1 =  A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ])\n",
    "val_aug2 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ])\n",
    "val_aug3 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(448, 448),\n",
    "    ])\n",
    "val_aug4 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(768, 768),\n",
    "    ])\n",
    "val_aug5 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(484, 484),\n",
    "    ])\n",
    "val_aug6 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(484, 484),\n",
    "    ])\n",
    "val_aug7 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(484, 484),\n",
    "    ])\n",
    "val_aug8 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(484, 484),\n",
    "    ])\n",
    "val_aug9 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(484, 484),\n",
    "    ])\n",
    "val_aug10 = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # A.ImageCompression(quality_lower=99, quality_upper=100),    \n",
    "        # A.Resize(484, 484),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, valid_loader):\n",
    "    from tqdm import tqdm\n",
    "    model.eval()\n",
    "    val_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            batch['input'] = batch['input'].to(args.device)\n",
    "\n",
    "            output_dict = model(batch, get_embeddings=True)\n",
    "            logits = output_dict['logits']\n",
    "            embeddings = output_dict['embeddings']   \n",
    "\n",
    "            output = dict({\n",
    "                'idx':batch['idx'].detach().cpu().numpy().astype(int),\n",
    "                'embeddings': embeddings.detach().cpu().numpy(),\n",
    "                \n",
    "            })\n",
    "            val_outputs += [output] \n",
    "    return val_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "sub = pd.read_csv('../data/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       path  label\n",
       "0   ../data/train/953/5.jpg    953\n",
       "1   ../data/train/953/3.jpg    953\n",
       "2  ../data/train/953/40.jpg    953\n",
       "3  ../data/train/953/29.jpg    953\n",
       "4  ../data/train/953/10.jpg    953"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../data/train/953/5.jpg</td>\n      <td>953</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../data/train/953/3.jpg</td>\n      <td>953</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../data/train/953/40.jpg</td>\n      <td>953</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../data/train/953/29.jpg</td>\n      <td>953</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../data/train/953/10.jpg</td>\n      <td>953</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  filename  prediction\n",
       "0    0.jpg         NaN\n",
       "1    1.jpg         NaN\n",
       "2    2.jpg         NaN\n",
       "3    3.jpg         NaN\n",
       "4    4.jpg         NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.jpg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.jpg</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['filepath'] = [os.path.join('../data/test', id) for id in sub['filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "train['filepath'] = train['path']\n",
    "sub = pd.read_csv('../data/sample.csv')\n",
    "sub['filepath'] = [os.path.join('../data/test', id) for id in sub['filename']]\n",
    "\n",
    "train_dataset = LMDataset(train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, num_workers=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "test_dataset = LMDataset(sub)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=128, num_workers=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model1 = Net(config0)\n",
    "model2 = Net(config1)\n",
    "model3 = Net(config2)\n",
    "model4 = Net(config3)\n",
    "model5 = Net(config4)\n",
    "model6 = Net(config6)\n",
    "model7 = Net(config6)\n",
    "model8 = Net(config7)\n",
    "model9 = Net(config8)\n",
    "model10 = Net(config9)\n",
    "\n",
    "\n",
    "model1 = model1.to('cuda:0')\n",
    "model2 = model2.to('cuda:0')\n",
    "model3 = model3.to('cuda:0')\n",
    "model4 = model4.to('cuda:0')\n",
    "model5 = model5.to('cuda:0')\n",
    "model6 = model6.to('cuda:0')\n",
    "model7 = model7.to('cuda:0')\n",
    "model8 = model8.to('cuda:0')\n",
    "model9 = model9.to('cuda:0')\n",
    "model10 = model10.to('cuda:0')\n",
    "\n",
    "\n",
    "model1.load_state_dict(torch.load(config0.model))\n",
    "model2.load_state_dict(torch.load(config1.model))\n",
    "model3.load_state_dict(torch.load(config2.model))\n",
    "model4.load_state_dict(torch.load(config3.model))\n",
    "model5.load_state_dict(torch.load(config4.model))\n",
    "model6.load_state_dict(torch.load(config5.model))\n",
    "model7.load_state_dict(torch.load(config6.model))\n",
    "model8.load_state_dict(torch.load(config7.model))\n",
    "model9.load_state_dict(torch.load(config8.model))\n",
    "model10.load_state_dict(torch.load(config9.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 375/375 [17:02<00:00,  2.73s/it]\n",
      "100%|██████████| 563/563 [25:28<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "CLS_TOP_K=5\n",
    "TOP_K=5\n",
    "# model1.eval()\n",
    "# model2.eval()\n",
    "# model3.eval()\n",
    "# model4.eval()\n",
    "# model5.eval()\n",
    "# model6.eval()\n",
    "# model7.eval()\n",
    "# model8.eval()\n",
    "# model9.eval()\n",
    "# model10.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = []\n",
    "    for batch, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10 in tqdm(train_loader):\n",
    "        batch['input'] = batch['input'].to('cuda:0')\n",
    "        batch2['input'] = batch2['input'].to('cuda:0')\n",
    "        batch3['input'] = batch3['input'].to('cuda:0')\n",
    "        batch4['input'] = batch4['input'].to('cuda:0')\n",
    "        batch5['input'] = batch5['input'].to('cuda:0')\n",
    "        batch6['input'] = batch6['input'].to('cuda:0')\n",
    "        batch7['input'] = batch7['input'].to('cuda:0')\n",
    "        batch8['input'] = batch8['input'].to('cuda:0')\n",
    "        batch9['input'] = batch9['input'].to('cuda:0')\n",
    "        batch10['input'] = batch10['input'].to('cuda:0')\n",
    "        \n",
    "\n",
    "        \n",
    "        output_dict1 = model1(batch, get_embeddings=True)\n",
    "        output_dict2 = model2(batch, get_embeddings=True)\n",
    "        output_dict3 = model3(batch, get_embeddings=True)\n",
    "        output_dict4 = model4(batch, get_embeddings=True)\n",
    "        output_dict5 = model5(batch, get_embeddings=True)\n",
    "        output_dict6 = model6(batch, get_embeddings=True)\n",
    "        output_dict7 = model7(batch, get_embeddings=True)\n",
    "        output_dict8 = model8(batch, get_embeddings=True)\n",
    "        output_dict9 = model9(batch, get_embeddings=True)\n",
    "        output_dict10 = model10(batch, get_embeddings=True)\n",
    "\n",
    "\n",
    "        \n",
    "        feat = torch.cat([\n",
    "            output_dict1['embeddings'], \n",
    "        output_dict2['embeddings'], \n",
    "        output_dict3['embeddings'], \n",
    "        output_dict4['embeddings'], \n",
    "        output_dict5['embeddings'], \n",
    "        output_dict6['embeddings'], \n",
    "        output_dict7['embeddings'], \n",
    "        output_dict8['embeddings'], \n",
    "        output_dict9['embeddings'], \n",
    "        output_dict10['embeddings'],\n",
    "         ], dim=1) \n",
    "        feats.append(feat.detach().cpu())\n",
    "    else:\n",
    "        feats = torch.cat(feats)\n",
    "        feats = feats.to('cuda:0')\n",
    "        feats = F.normalize(feats)\n",
    "        \n",
    "    PRODS = []\n",
    "    PREDS = []\n",
    "    PRODS_M = []\n",
    "    PREDS_M = []     \n",
    "    \n",
    "    for batch, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10 in tqdm(test_loader):\n",
    "        batch['input'] = batch['input'].to('cuda:0')\n",
    "        batch2['input'] = batch2['input'].to('cuda:0')\n",
    "        batch3['input'] = batch3['input'].to('cuda:0')\n",
    "        batch4['input'] = batch4['input'].to('cuda:0')\n",
    "        batch5['input'] = batch5['input'].to('cuda:0')\n",
    "        batch6['input'] = batch6['input'].to('cuda:0')\n",
    "        batch7['input'] = batch7['input'].to('cuda:0')\n",
    "        batch8['input'] = batch8['input'].to('cuda:0')\n",
    "        batch9['input'] = batch9['input'].to('cuda:0')\n",
    "        batch10['input'] = batch10['input'].to('cuda:0')\n",
    "       \n",
    "        \n",
    "\n",
    "        output_dict = model1(batch,  get_embeddings=True) ; logits =  output_dict['logits'] ; embeddings1 = output_dict['embeddings']\n",
    "        output_dict = model2(batch2, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings2 = output_dict['embeddings']\n",
    "        output_dict = model3(batch3, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings3 = output_dict['embeddings']\n",
    "        output_dict = model4(batch4, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings4 = output_dict['embeddings']\n",
    "        output_dict = model5(batch5, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings5 = output_dict['embeddings']\n",
    "        output_dict = model6(batch6, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings6 = output_dict['embeddings']\n",
    "        output_dict = model7(batch7, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings7 = output_dict['embeddings']\n",
    "        output_dict = model8(batch8, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings8 = output_dict['embeddings']\n",
    "        output_dict = model9(batch9, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings9 = output_dict['embeddings']\n",
    "        output_dict = model10(batch10, get_embeddings=True) ; logits += output_dict['logits'] ; embeddings10 = output_dict['embeddings']\n",
    "        \n",
    "\n",
    "\n",
    "        feat = torch.cat([embeddings1, embeddings2, embeddings3, embeddings4, embeddings5, embeddings6, embeddings7, embeddings8, embeddings9, embeddings10], dim=1)\n",
    "        feat = F.normalize(feat)\n",
    "        \n",
    "        logits=logits/9\n",
    "        \n",
    "        (values, indices) = torch.topk(logits, CLS_TOP_K, dim=1)\n",
    "        probs_m = values\n",
    "        preds_m = indices  \n",
    "        \n",
    "        PRODS_M.append(probs_m.detach().cpu())\n",
    "        PREDS_M.append(preds_m.detach().cpu())  \n",
    "        \n",
    "        distance = feat.mm(feats.t())\n",
    "        (values, indices) = torch.topk(distance, TOP_K, dim=1)\n",
    "        probs = values\n",
    "        preds = indices\n",
    "        PRODS.append(probs.detach().cpu())\n",
    "        PREDS.append(preds.detach().cpu())\n",
    "        \n",
    "    PRODS = torch.cat(PRODS).numpy()\n",
    "    PREDS = torch.cat(PREDS).numpy()\n",
    "    PRODS_M = torch.cat(PRODS_M).numpy()\n",
    "    PREDS_M = torch.cat(PREDS_M).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2355,  2360,  2393,  2399,  2396],\n",
       "       [ 4093,  4092,  4095,  4090,  4088],\n",
       "       [37946, 37929, 37932, 37952, 37945],\n",
       "       ...,\n",
       "       [47642, 47650, 47634, 47658, 47648],\n",
       "       [46711, 46739, 46706, 46713, 46707],\n",
       "       [ 6066,  6082,  6083,  6074,  6050]])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.7511135 , 0.1784661 , 0.15747197, 0.133756  , 0.13362974],\n",
       "       [0.73687226, 0.17923696, 0.1766392 , 0.171577  , 0.1630468 ],\n",
       "       [0.69385415, 0.19517149, 0.177784  , 0.1560062 , 0.14576189],\n",
       "       ...,\n",
       "       [0.73839927, 0.21938832, 0.1890858 , 0.17360687, 0.16662918],\n",
       "       [0.4927984 , 0.2682632 , 0.21025248, 0.20141374, 0.18291219],\n",
       "       [0.8288091 , 0.17472574, 0.16799076, 0.14628673, 0.14221893]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "PRODS_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[494, 213, 725, 503, 936],\n",
       "       [208, 227, 215, 754, 385],\n",
       "       [114, 669,  96, 111, 233],\n",
       "       ...,\n",
       "       [311, 465, 955, 363, 931],\n",
       "       [419, 613, 433, 916, 740],\n",
       "       [748, 706, 574, 240, 801]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "PREDS_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ensembles/PRODS.npy', PRODS)\n",
    "np.save('../ensembles/PREDS.npy', PREDS)\n",
    "np.save('../ensembles/PREDS_M.npy', PREDS_M)\n",
    "np.save('../ensembles/PREDS_M.npy', PREDS_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark = train['label'].values\n",
    "PREDS2 = landmark[PREDS]\n",
    "# PREDS_M = np.vectorize(idx2landmark_id.get)(PREDS_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 72000/72000 [00:01<00:00, 50507.53it/s]\n"
     ]
    }
   ],
   "source": [
    "PRODS_F = []\n",
    "PREDS_F = []\n",
    "for i in tqdm(range(PREDS2.shape[0])):\n",
    "    tmp = {}\n",
    "    classify_dict = {PREDS_M[i,j] : PRODS_M[i,j] for j in range(CLS_TOP_K)}\n",
    "    for k in range(TOP_K):\n",
    "        lid = PREDS2[i, k]\n",
    "        tmp[lid] = tmp.get(lid, 0.) + float(PRODS[i, k]) ** 9 * classify_dict.get(lid,1e-8)**10\n",
    "    pred, conf = max(tmp.items(), key=lambda x: x[1])\n",
    "    PREDS_F.append(pred)\n",
    "    PRODS_F.append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['prediction']=PREDS_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  filename  prediction            filepath\n",
       "0    0.jpg         494  ../data/test/0.jpg\n",
       "1    1.jpg         208  ../data/test/1.jpg\n",
       "2    2.jpg         114  ../data/test/2.jpg\n",
       "3    3.jpg         643  ../data/test/3.jpg\n",
       "4    4.jpg         201  ../data/test/4.jpg"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>prediction</th>\n      <th>filepath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>494</td>\n      <td>../data/test/0.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.jpg</td>\n      <td>208</td>\n      <td>../data/test/1.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>114</td>\n      <td>../data/test/2.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.jpg</td>\n      <td>643</td>\n      <td>../data/test/3.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.jpg</td>\n      <td>201</td>\n      <td>../data/test/4.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.drop(columns=['filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../submit/baseline_19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}